created virtual environment CPython3.7.7.final.0-64 in 298ms
  creator CPython3Posix(dest=/localscratch/helenr6.6279148.0/env, clear=False, global=False)
  seeder FromAppData(download=False, pip=latest, setuptools=latest, wheel=latest, via=copy, app_data_dir=/home/helenr6/.local/share/virtualenv/seed-app-data/v1.0.1)
  activators BashActivator,CShellActivator,FishActivator,PowerShellActivator,PythonActivator,XonshActivator
Ignoring pip: markers 'python_version < "3"' don't match your environment
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/pip-21.2.3+computecanada-py3-none-any.whl
Installing collected packages: pip
  Attempting uninstall: pip
    Found existing installation: pip 20.0.2
    Uninstalling pip-20.0.2:
      Successfully uninstalled pip-20.0.2
Successfully installed pip-21.2.3+computecanada
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic/torchvision-0.11.1+computecanada-cp37-cp37m-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2/Pillow_SIMD-7.0.0.post3+computecanada-cp37-cp37m-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic/numpy-1.21.4+computecanada-cp37-cp37m-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2/torch-1.10.0+computecanada-cp37-cp37m-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/typing_extensions-4.2.0+computecanada-py3-none-any.whl
Installing collected packages: typing-extensions, torch, pillow-simd, numpy, torchvision
Successfully installed numpy-1.21.4+computecanada pillow-simd-7.0.0.post3+computecanada torch-1.10.0+computecanada torchvision-0.11.1+computecanada typing-extensions-4.2.0+computecanada
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Requirement already satisfied: torch in /localscratch/helenr6.6279148.0/env/lib/python3.7/site-packages (1.10.0+computecanada)
Requirement already satisfied: typing-extensions in /localscratch/helenr6.6279148.0/env/lib/python3.7/site-packages (from torch) (4.2.0+computecanada)
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/tensorboard-2.8.0+computecanada-py3-none-any.whl
Requirement already satisfied: numpy>=1.12.0 in /localscratch/helenr6.6279148.0/env/lib/python3.7/site-packages (from tensorboard) (1.21.4+computecanada)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/tensorboard_data_server-0.6.1+computecanada-py3-none-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/protobuf-3.19.4+computecanada-py2.py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/requests-2.27.1+computecanada-py2.py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/absl_py-1.0.0+computecanada-py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/Werkzeug-2.1.2+computecanada-py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/google_auth_oauthlib-0.4.6+computecanada-py2.py3-none-any.whl
Requirement already satisfied: wheel>=0.26 in /localscratch/helenr6.6279148.0/env/lib/python3.7/site-packages (from tensorboard) (0.34.2)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic/grpcio-1.38.1+computecanada-cp37-cp37m-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/Markdown-3.3.6+computecanada-py3-none-any.whl
Requirement already satisfied: setuptools>=41.0.0 in /localscratch/helenr6.6279148.0/env/lib/python3.7/site-packages (from tensorboard) (46.1.3)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/tensorboard_plugin_wit-1.8.0+computecanada-py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/google_auth-2.3.3+computecanada-py2.py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/six-1.16.0+computecanada-py2.py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/rsa-4.8+computecanada-py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/pyasn1_modules-0.2.8+computecanada-py2.py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/cachetools-4.2.4+computecanada-py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/requests_oauthlib-1.3.0+computecanada-py2.py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/importlib_metadata-4.11.3+computecanada-py3-none-any.whl
Requirement already satisfied: typing-extensions>=3.6.4 in /localscratch/helenr6.6279148.0/env/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (4.2.0+computecanada)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/zipp-3.8.0+computecanada-py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/pyasn1-0.4.8+computecanada-py2.py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/certifi-2021.10.8+computecanada-py2.py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/charset_normalizer-2.0.12+computecanada-py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/urllib3-1.26.9+computecanada-py2.py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/idna-3.3+computecanada-py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/oauthlib-3.1.1+computecanada-py2.py3-none-any.whl
Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, zipp, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, tensorboard
Successfully installed absl-py-1.0.0+computecanada cachetools-4.2.4+computecanada certifi-2021.10.8+computecanada charset-normalizer-2.0.12+computecanada google-auth-2.3.3+computecanada google-auth-oauthlib-0.4.6+computecanada grpcio-1.38.1+computecanada idna-3.3+computecanada importlib-metadata-4.11.3+computecanada markdown-3.3.6+computecanada oauthlib-3.1.1+computecanada protobuf-3.19.4+computecanada pyasn1-0.4.8+computecanada pyasn1-modules-0.2.8+computecanada requests-2.27.1+computecanada requests-oauthlib-1.3.0+computecanada rsa-4.8+computecanada six-1.16.0+computecanada tensorboard-2.8.0+computecanada tensorboard-data-server-0.6.1+computecanada tensorboard-plugin-wit-1.8.0+computecanada urllib3-1.26.9+computecanada werkzeug-2.1.2+computecanada zipp-3.8.0+computecanada
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2/h5py-3.4.0+computecanada-cp37-cp37m-linux_x86_64.whl
Requirement already satisfied: numpy>=1.14.5 in /localscratch/helenr6.6279148.0/env/lib/python3.7/site-packages (from h5py) (1.21.4+computecanada)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/cached_property-1.5.2+computecanada-py2.py3-none-any.whl
Installing collected packages: cached-property, h5py
Successfully installed cached-property-1.5.2+computecanada h5py-3.4.0+computecanada
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2/matplotlib-3.4.2+computecanada-cp37-cp37m-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/python_dateutil-2.8.2+computecanada-py2.py3-none-any.whl
Requirement already satisfied: numpy>=1.16 in /localscratch/helenr6.6279148.0/env/lib/python3.7/site-packages (from matplotlib) (1.21.4+computecanada)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/pyparsing-3.0.8+computecanada-py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2/Pillow-8.4.0+computecanada-cp37-cp37m-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/cycler-0.11.0+computecanada-py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2/kiwisolver-1.3.1+computecanada-cp37-cp37m-linux_x86_64.whl
Requirement already satisfied: six>=1.5 in /localscratch/helenr6.6279148.0/env/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0+computecanada)
Installing collected packages: python-dateutil, pyparsing, pillow, kiwisolver, cycler, matplotlib
Successfully installed cycler-0.11.0+computecanada kiwisolver-1.3.1+computecanada matplotlib-3.4.2+computecanada pillow-8.4.0+computecanada pyparsing-3.0.8+computecanada python-dateutil-2.8.2+computecanada
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/sklearn-0.0+computecanada-py2.py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic/scikit_learn-1.0.1+computecanada-cp37-cp37m-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/threadpoolctl-3.0.0+computecanada-py3-none-any.whl
Requirement already satisfied: numpy>=1.14.6 in /localscratch/helenr6.6279148.0/env/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.21.4+computecanada)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/joblib-1.1.0+computecanada-py2.py3-none-any.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic/scipy-1.7.3+computecanada-cp37-cp37m-linux_x86_64.whl
Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sklearn
Successfully installed joblib-1.1.0+computecanada scikit-learn-1.0.1+computecanada scipy-1.7.3+computecanada sklearn-0.0+computecanada threadpoolctl-3.0.0+computecanada
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2/pandas-1.3.5+computecanada-cp37-cp37m-linux_x86_64.whl
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/pytz-2022.1+computecanada-py3-none-any.whl
Requirement already satisfied: python-dateutil>=2.7.3 in /localscratch/helenr6.6279148.0/env/lib/python3.7/site-packages (from pandas) (2.8.2+computecanada)
Requirement already satisfied: numpy>=1.17.3 in /localscratch/helenr6.6279148.0/env/lib/python3.7/site-packages (from pandas) (1.21.4+computecanada)
Requirement already satisfied: six>=1.5 in /localscratch/helenr6.6279148.0/env/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0+computecanada)
Installing collected packages: pytz, pandas
Successfully installed pandas-1.3.5+computecanada pytz-2022.1+computecanada
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /localscratch/helenr6.6279148.0/model_setup
  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.
   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.
Building wheels for collected packages: load-model
  Building wheel for load-model (setup.py): started
  Building wheel for load-model (setup.py): finished with status 'done'
  Created wheel for load-model: filename=load_model-0.1.0-py3-none-any.whl size=28532 sha256=7a313d65641726abc85ac20f52603257e442b378ee9278d672bd1df04d13030b
  Stored in directory: /tmp/pip-ephem-wheel-cache-9ot25mcd/wheels/19/93/00/6a820facf86e022375d7dee5d2d195a7e85383ce6e80de82cd
Successfully built load-model
Installing collected packages: load-model
Successfully installed load-model-0.1.0
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /localscratch/helenr6.6279148.0/advertorch
  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.
   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.
Building wheels for collected packages: advertorch
  Building wheel for advertorch (setup.py): started
  Building wheel for advertorch (setup.py): finished with status 'done'
  Created wheel for advertorch: filename=advertorch-0.2.4-py3-none-any.whl size=5694836 sha256=7cd6a207f4a8b8ebb370825555746303a2b309933273015da629d1676d06f17e
  Stored in directory: /tmp/pip-ephem-wheel-cache-34bdys8k/wheels/96/93/ad/74ea79bfb1ac4cbd407f17415cfe2673f912d2069980c669dc
Successfully built advertorch
Installing collected packages: advertorch
Successfully installed advertorch-0.2.4
=> using pre-trained model 'st_resnet'
s/stretch/session_1
torch.Size([640, 3, 224, 224])
(640, 22)
(64, 1024, 14, 14)
(128, 1024, 14, 14)
(192, 1024, 14, 14)
(256, 1024, 14, 14)
(320, 1024, 14, 14)
(384, 1024, 14, 14)
(448, 1024, 14, 14)
(512, 1024, 14, 14)
(576, 1024, 14, 14)
attach reg
0
TRAINING
/home/helenr6/Adv_mapper/train_split.py:125: UserWarning: You have chosen to seed training. This will turn on the CUDNN deterministic setting, which can slow down your training considerably! You may see unexpected behavior when restarting from checkpoints.
  warnings.warn('You have chosen to seed training. '
/localscratch/helenr6.6279148.0/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
Epoch: [0][0/1]	Time  5.847 ( 5.847)	Data  1.770 ( 1.770)	Loss 1.7173e+00 (1.7173e+00)	Acc@1   0.02 (  0.02)	Acc@5   0.00 (  0.00)
[0.01763665053071443]
Test: [0/1]	Time  0.469 ( 0.469)	Loss 2.4604e+00 (2.4604e+00)	Acc@1   0.14 (  0.14)	Acc@5   0.00 (  0.00)
 * Acc@1 0.137 Acc@5 0.000
Test: [0/1]	Time  0.605 ( 0.605)	Loss 1.0359e+00 (1.0359e+00)	Acc@1  -0.00 ( -0.00)	Acc@5   0.00 (  0.00)
 * Acc@1 -0.004 Acc@5 0.000
1
TRAINING
Epoch: [1][0/1]	Time  1.115 ( 1.115)	Data  0.219 ( 0.219)	Loss 1.6438e+00 (1.6438e+00)	Acc@1   0.02 (  0.02)	Acc@5   0.00 (  0.00)
[0.021755188333027007]
Test: [0/1]	Time  0.251 ( 0.251)	Loss 2.4375e+00 (2.4375e+00)	Acc@1   0.14 (  0.14)	Acc@5   0.00 (  0.00)
 * Acc@1 0.142 Acc@5 0.000
Test: [0/1]	Time  0.317 ( 0.317)	Loss 9.7398e-01 (9.7398e-01)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
 * Acc@1 0.001 Acc@5 0.000
2
TRAINING
Epoch: [2][0/1]	Time  0.944 ( 0.944)	Data  0.219 ( 0.219)	Loss 1.5312e+00 (1.5312e+00)	Acc@1   0.03 (  0.03)	Acc@5   0.00 (  0.00)
[0.029672319645380155]
Test: [0/1]	Time  0.250 ( 0.250)	Loss 2.4069e+00 (2.4069e+00)	Acc@1   0.15 (  0.15)	Acc@5   0.00 (  0.00)
 * Acc@1 0.149 Acc@5 0.000
Test: [0/1]	Time  0.314 ( 0.314)	Loss 9.1294e-01 (9.1294e-01)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
 * Acc@1 0.004 Acc@5 0.000
3
TRAINING
Epoch: [3][0/1]	Time  0.984 ( 0.984)	Data  0.249 ( 0.249)	Loss 1.4180e+00 (1.4180e+00)	Acc@1   0.04 (  0.04)	Acc@5   0.00 (  0.00)
[0.04114678959554151]
Test: [0/1]	Time  0.256 ( 0.256)	Loss 2.3709e+00 (2.3709e+00)	Acc@1   0.16 (  0.16)	Acc@5   0.00 (  0.00)
 * Acc@1 0.163 Acc@5 0.000
Test: [0/1]	Time  0.324 ( 0.324)	Loss 8.6825e-01 (8.6825e-01)	Acc@1   0.01 (  0.01)	Acc@5   0.00 (  0.00)
 * Acc@1 0.009 Acc@5 0.000
4
TRAINING
Epoch: [4][0/1]	Time  0.950 ( 0.950)	Data  0.226 ( 0.226)	Loss 1.3271e+00 (1.3271e+00)	Acc@1   0.05 (  0.05)	Acc@5   0.00 (  0.00)
[0.054098496442913875]
Test: [0/1]	Time  0.253 ( 0.253)	Loss 2.3316e+00 (2.3316e+00)	Acc@1   0.18 (  0.18)	Acc@5   0.00 (  0.00)
 * Acc@1 0.185 Acc@5 0.000
Test: [0/1]	Time  0.317 ( 0.317)	Loss 8.4003e-01 (8.4003e-01)	Acc@1   0.02 (  0.02)	Acc@5   0.00 (  0.00)
 * Acc@1 0.018 Acc@5 0.000
5
TRAINING
Epoch: [5][0/1]	Time  0.963 ( 0.963)	Data  0.226 ( 0.226)	Loss 1.2578e+00 (1.2578e+00)	Acc@1   0.07 (  0.07)	Acc@5   0.00 (  0.00)
[0.06699682100496027]
Test: [0/1]	Time  0.253 ( 0.253)	Loss 2.2908e+00 (2.2908e+00)	Acc@1   0.23 (  0.23)	Acc@5   0.00 (  0.00)
 * Acc@1 0.229 Acc@5 0.000
Test: [0/1]	Time  0.316 ( 0.316)	Loss 8.1721e-01 (8.1721e-01)	Acc@1   0.03 (  0.03)	Acc@5   0.00 (  0.00)
 * Acc@1 0.029 Acc@5 0.000
6
TRAINING
Epoch: [6][0/1]	Time  0.964 ( 0.964)	Data  0.227 ( 0.227)	Loss 1.1931e+00 (1.1931e+00)	Acc@1   0.08 (  0.08)	Acc@5   0.00 (  0.00)
[0.08241433437322265]
Test: [0/1]	Time  0.249 ( 0.249)	Loss 2.2505e+00 (2.2505e+00)	Acc@1   0.27 (  0.27)	Acc@5   0.00 (  0.00)
 * Acc@1 0.266 Acc@5 0.000
Test: [0/1]	Time  0.318 ( 0.318)	Loss 7.8688e-01 (7.8688e-01)	Acc@1   0.04 (  0.04)	Acc@5   0.00 (  0.00)
 * Acc@1 0.042 Acc@5 0.000
7
TRAINING
Epoch: [7][0/1]	Time  0.945 ( 0.945)	Data  0.219 ( 0.219)	Loss 1.1145e+00 (1.1145e+00)	Acc@1   0.10 (  0.10)	Acc@5   0.00 (  0.00)
[0.10079544224843531]
Test: [0/1]	Time  0.250 ( 0.250)	Loss 2.2113e+00 (2.2113e+00)	Acc@1   0.30 (  0.30)	Acc@5   0.00 (  0.00)
 * Acc@1 0.296 Acc@5 0.000
Test: [0/1]	Time  0.313 ( 0.313)	Loss 7.4288e-01 (7.4288e-01)	Acc@1   0.05 (  0.05)	Acc@5   0.00 (  0.00)
 * Acc@1 0.053 Acc@5 0.000
8
TRAINING
Epoch: [8][0/1]	Time  0.970 ( 0.970)	Data  0.229 ( 0.229)	Loss 1.0137e+00 (1.0137e+00)	Acc@1   0.12 (  0.12)	Acc@5   0.00 (  0.00)
[0.12184897356711968]
Test: [0/1]	Time  0.254 ( 0.254)	Loss 2.1743e+00 (2.1743e+00)	Acc@1   0.36 (  0.36)	Acc@5   0.00 (  0.00)
 * Acc@1 0.363 Acc@5 0.000
Test: [0/1]	Time  0.315 ( 0.315)	Loss 6.8866e-01 (6.8866e-01)	Acc@1   0.06 (  0.06)	Acc@5   0.00 (  0.00)
 * Acc@1 0.059 Acc@5 0.000
9
TRAINING
Epoch: [9][0/1]	Time  0.957 ( 0.957)	Data  0.225 ( 0.225)	Loss 8.9741e-01 (8.9741e-01)	Acc@1   0.14 (  0.14)	Acc@5   0.00 (  0.00)
[0.14257457810408658]
Test: [0/1]	Time  0.249 ( 0.249)	Loss 2.1397e+00 (2.1397e+00)	Acc@1   0.39 (  0.39)	Acc@5   0.00 (  0.00)
 * Acc@1 0.395 Acc@5 0.000
Test: [0/1]	Time  0.325 ( 0.325)	Loss 6.3386e-01 (6.3386e-01)	Acc@1   0.07 (  0.07)	Acc@5   0.00 (  0.00)
 * Acc@1 0.068 Acc@5 0.000
10
TRAINING
Epoch: [10][0/1]	Time  0.957 ( 0.957)	Data  0.217 ( 0.217)	Loss 7.8138e-01 (7.8138e-01)	Acc@1   0.16 (  0.16)	Acc@5   0.00 (  0.00)
[0.16462292109985954]
Test: [0/1]	Time  0.248 ( 0.248)	Loss 2.1073e+00 (2.1073e+00)	Acc@1   0.43 (  0.43)	Acc@5   0.00 (  0.00)
 * Acc@1 0.429 Acc@5 0.000
Test: [0/1]	Time  0.317 ( 0.317)	Loss 5.8784e-01 (5.8784e-01)	Acc@1   0.08 (  0.08)	Acc@5   0.00 (  0.00)
 * Acc@1 0.082 Acc@5 0.000
11
TRAINING
Epoch: [11][0/1]	Time  0.974 ( 0.974)	Data  0.226 ( 0.226)	Loss 6.8083e-01 (6.8083e-01)	Acc@1   0.19 (  0.19)	Acc@5   0.00 (  0.00)
[0.1860897057435068]
Test: [0/1]	Time  0.248 ( 0.248)	Loss 2.0770e+00 (2.0770e+00)	Acc@1   0.45 (  0.45)	Acc@5   0.00 (  0.00)
 * Acc@1 0.455 Acc@5 0.000
Test: [0/1]	Time  0.314 ( 0.314)	Loss 5.5433e-01 (5.5433e-01)	Acc@1   0.10 (  0.10)	Acc@5   0.00 (  0.00)
 * Acc@1 0.101 Acc@5 0.000
12
TRAINING
Epoch: [12][0/1]	Time  1.022 ( 1.022)	Data  0.283 ( 0.283)	Loss 6.0258e-01 (6.0258e-01)	Acc@1   0.21 (  0.21)	Acc@5   0.00 (  0.00)
[0.20726179846197673]
Test: [0/1]	Time  0.252 ( 0.252)	Loss 2.0483e+00 (2.0483e+00)	Acc@1   0.53 (  0.53)	Acc@5   0.00 (  0.00)
 * Acc@1 0.530 Acc@5 0.000
Test: [0/1]	Time  0.312 ( 0.312)	Loss 5.3013e-01 (5.3013e-01)	Acc@1   0.12 (  0.12)	Acc@5   0.00 (  0.00)
 * Acc@1 0.125 Acc@5 0.000
13
TRAINING
Epoch: [13][0/1]	Time  1.024 ( 1.024)	Data  0.276 ( 0.276)	Loss 5.4309e-01 (5.4309e-01)	Acc@1   0.23 (  0.23)	Acc@5   0.00 (  0.00)
[0.23389610723210089]
Test: [0/1]	Time  0.249 ( 0.249)	Loss 2.0208e+00 (2.0208e+00)	Acc@1   0.56 (  0.56)	Acc@5   0.00 (  0.00)
 * Acc@1 0.558 Acc@5 0.000
Test: [0/1]	Time  0.313 ( 0.313)	Loss 5.0789e-01 (5.0789e-01)	Acc@1   0.15 (  0.15)	Acc@5   0.00 (  0.00)
 * Acc@1 0.150 Acc@5 0.000
14
TRAINING
Epoch: [14][0/1]	Time  1.017 ( 1.017)	Data  0.268 ( 0.268)	Loss 4.9259e-01 (4.9259e-01)	Acc@1   0.26 (  0.26)	Acc@5   0.00 (  0.00)
[0.2628378870535081]
Test: [0/1]	Time  0.251 ( 0.251)	Loss 1.9946e+00 (1.9946e+00)	Acc@1   0.58 (  0.58)	Acc@5   0.00 (  0.00)
 * Acc@1 0.577 Acc@5 0.000
Test: [0/1]	Time  0.314 ( 0.314)	Loss 4.8085e-01 (4.8085e-01)	Acc@1   0.18 (  0.18)	Acc@5   0.00 (  0.00)
 * Acc@1 0.178 Acc@5 0.000
15
TRAINING
Epoch: [15][0/1]	Time  0.952 ( 0.952)	Data  0.221 ( 0.221)	Loss 4.4170e-01 (4.4170e-01)	Acc@1   0.31 (  0.31)	Acc@5   0.00 (  0.00)
[0.3079522527962675]
Test: [0/1]	Time  0.255 ( 0.255)	Loss 1.9695e+00 (1.9695e+00)	Acc@1   0.61 (  0.61)	Acc@5   0.00 (  0.00)
 * Acc@1 0.614 Acc@5 0.000
Test: [0/1]	Time  0.314 ( 0.314)	Loss 4.4649e-01 (4.4649e-01)	Acc@1   0.21 (  0.21)	Acc@5   0.00 (  0.00)
 * Acc@1 0.207 Acc@5 0.000
16
TRAINING
Epoch: [16][0/1]	Time  0.961 ( 0.961)	Data  0.224 ( 0.224)	Loss 3.8657e-01 (3.8657e-01)	Acc@1   0.35 (  0.35)	Acc@5   0.00 (  0.00)
[0.35372352185700406]
Test: [0/1]	Time  0.249 ( 0.249)	Loss 1.9454e+00 (1.9454e+00)	Acc@1   0.65 (  0.65)	Acc@5   0.00 (  0.00)
 * Acc@1 0.652 Acc@5 0.000
Test: [0/1]	Time  0.313 ( 0.313)	Loss 4.0729e-01 (4.0729e-01)	Acc@1   0.24 (  0.24)	Acc@5   0.00 (  0.00)
 * Acc@1 0.236 Acc@5 0.000
17
TRAINING
Epoch: [17][0/1]	Time  0.968 ( 0.968)	Data  0.220 ( 0.220)	Loss 3.2982e-01 (3.2982e-01)	Acc@1   0.40 (  0.40)	Acc@5   0.00 (  0.00)
[0.3982482616602827]
Test: [0/1]	Time  0.250 ( 0.250)	Loss 1.9226e+00 (1.9226e+00)	Acc@1   0.67 (  0.67)	Acc@5   0.00 (  0.00)
 * Acc@1 0.674 Acc@5 0.000
Test: [0/1]	Time  0.321 ( 0.321)	Loss 3.6863e-01 (3.6863e-01)	Acc@1   0.26 (  0.26)	Acc@5   0.00 (  0.00)
 * Acc@1 0.256 Acc@5 0.000
18
TRAINING
Epoch: [18][0/1]	Time  0.964 ( 0.964)	Data  0.239 ( 0.239)	Loss 2.7775e-01 (2.7775e-01)	Acc@1   0.45 (  0.45)	Acc@5   0.00 (  0.00)
[0.44650296055887007]
Test: [0/1]	Time  0.255 ( 0.255)	Loss 1.9014e+00 (1.9014e+00)	Acc@1   0.70 (  0.70)	Acc@5   0.00 (  0.00)
 * Acc@1 0.700 Acc@5 0.000
Test: [0/1]	Time  0.320 ( 0.320)	Loss 3.3547e-01 (3.3547e-01)	Acc@1   0.28 (  0.28)	Acc@5   0.00 (  0.00)
 * Acc@1 0.275 Acc@5 0.000
19
TRAINING
Epoch: [19][0/1]	Time  0.981 ( 0.981)	Data  0.228 ( 0.228)	Loss 2.3592e-01 (2.3592e-01)	Acc@1   0.49 (  0.49)	Acc@5   0.00 (  0.00)
[0.4922076133786105]
Test: [0/1]	Time  0.251 ( 0.251)	Loss 1.8820e+00 (1.8820e+00)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.712 Acc@5 0.000
Test: [0/1]	Time  0.314 ( 0.314)	Loss 3.0996e-01 (3.0996e-01)	Acc@1   0.29 (  0.29)	Acc@5   0.00 (  0.00)
 * Acc@1 0.290 Acc@5 0.000
20
TRAINING
Epoch: [20][0/1]	Time  0.967 ( 0.967)	Data  0.219 ( 0.219)	Loss 2.0604e-01 (2.0604e-01)	Acc@1   0.54 (  0.54)	Acc@5   0.00 (  0.00)
[0.538664168715548]
Test: [0/1]	Time  0.256 ( 0.256)	Loss 1.8645e+00 (1.8645e+00)	Acc@1   0.72 (  0.72)	Acc@5   0.00 (  0.00)
 * Acc@1 0.724 Acc@5 0.000
Test: [0/1]	Time  0.314 ( 0.314)	Loss 2.9091e-01 (2.9091e-01)	Acc@1   0.30 (  0.30)	Acc@5   0.00 (  0.00)
 * Acc@1 0.304 Acc@5 0.000
21
TRAINING
Epoch: [21][0/1]	Time  0.970 ( 0.970)	Data  0.222 ( 0.222)	Loss 1.8554e-01 (1.8554e-01)	Acc@1   0.59 (  0.59)	Acc@5   0.00 (  0.00)
[0.5852722889529027]
Test: [0/1]	Time  0.260 ( 0.260)	Loss 1.8491e+00 (1.8491e+00)	Acc@1   0.73 (  0.73)	Acc@5   0.00 (  0.00)
 * Acc@1 0.735 Acc@5 0.000
Test: [0/1]	Time  0.321 ( 0.321)	Loss 2.7526e-01 (2.7526e-01)	Acc@1   0.32 (  0.32)	Acc@5   0.00 (  0.00)
 * Acc@1 0.318 Acc@5 0.000
22
TRAINING
Epoch: [22][0/1]	Time  0.960 ( 0.960)	Data  0.223 ( 0.223)	Loss 1.6962e-01 (1.6962e-01)	Acc@1   0.63 (  0.63)	Acc@5   0.00 (  0.00)
[0.6312990786461015]
Test: [0/1]	Time  0.257 ( 0.257)	Loss 1.8357e+00 (1.8357e+00)	Acc@1   0.74 (  0.74)	Acc@5   0.00 (  0.00)
 * Acc@1 0.742 Acc@5 0.000
Test: [0/1]	Time  0.322 ( 0.322)	Loss 2.6011e-01 (2.6011e-01)	Acc@1   0.34 (  0.34)	Acc@5   0.00 (  0.00)
 * Acc@1 0.336 Acc@5 0.000
23
TRAINING
Epoch: [23][0/1]	Time  0.962 ( 0.962)	Data  0.230 ( 0.230)	Loss 1.5414e-01 (1.5414e-01)	Acc@1   0.67 (  0.67)	Acc@5   0.00 (  0.00)
[0.6737880108895641]
Test: [0/1]	Time  0.267 ( 0.267)	Loss 1.8245e+00 (1.8245e+00)	Acc@1   0.75 (  0.75)	Acc@5   0.00 (  0.00)
 * Acc@1 0.748 Acc@5 0.000
Test: [0/1]	Time  0.335 ( 0.335)	Loss 2.4426e-01 (2.4426e-01)	Acc@1   0.36 (  0.36)	Acc@5   0.00 (  0.00)
 * Acc@1 0.356 Acc@5 0.000
24
TRAINING
Epoch: [24][0/1]	Time  0.957 ( 0.957)	Data  0.233 ( 0.233)	Loss 1.3759e-01 (1.3759e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
[0.7143390390797577]
Test: [0/1]	Time  0.264 ( 0.264)	Loss 1.8151e+00 (1.8151e+00)	Acc@1   0.75 (  0.75)	Acc@5   0.00 (  0.00)
 * Acc@1 0.748 Acc@5 0.000
Test: [0/1]	Time  0.325 ( 0.325)	Loss 2.2849e-01 (2.2849e-01)	Acc@1   0.38 (  0.38)	Acc@5   0.00 (  0.00)
 * Acc@1 0.378 Acc@5 0.000
25
TRAINING
Epoch: [25][0/1]	Time  0.976 ( 0.976)	Data  0.233 ( 0.233)	Loss 1.2118e-01 (1.2118e-01)	Acc@1   0.75 (  0.75)	Acc@5   0.00 (  0.00)
[0.7525562400886819]
Test: [0/1]	Time  0.255 ( 0.255)	Loss 1.8076e+00 (1.8076e+00)	Acc@1   0.74 (  0.74)	Acc@5   0.00 (  0.00)
 * Acc@1 0.742 Acc@5 0.000
Test: [0/1]	Time  0.323 ( 0.323)	Loss 2.1454e-01 (2.1454e-01)	Acc@1   0.40 (  0.40)	Acc@5   0.00 (  0.00)
 * Acc@1 0.399 Acc@5 0.000
26
TRAINING
Epoch: [26][0/1]	Time  0.991 ( 0.991)	Data  0.245 ( 0.245)	Loss 1.0743e-01 (1.0743e-01)	Acc@1   0.78 (  0.78)	Acc@5   0.00 (  0.00)
[0.7841081289805915]
Test: [0/1]	Time  0.255 ( 0.255)	Loss 1.8017e+00 (1.8017e+00)	Acc@1   0.74 (  0.74)	Acc@5   0.00 (  0.00)
 * Acc@1 0.741 Acc@5 0.000
Test: [0/1]	Time  0.318 ( 0.318)	Loss 2.0375e-01 (2.0375e-01)	Acc@1   0.42 (  0.42)	Acc@5   0.00 (  0.00)
 * Acc@1 0.421 Acc@5 0.000
27
TRAINING
Epoch: [27][0/1]	Time  0.971 ( 0.971)	Data  0.234 ( 0.234)	Loss 9.8245e-02 (9.8245e-02)	Acc@1   0.81 (  0.81)	Acc@5   0.00 (  0.00)
[0.8083586247521646]
Test: [0/1]	Time  0.253 ( 0.253)	Loss 1.7976e+00 (1.7976e+00)	Acc@1   0.75 (  0.75)	Acc@5   0.00 (  0.00)
 * Acc@1 0.751 Acc@5 0.000
Test: [0/1]	Time  0.319 ( 0.319)	Loss 1.9624e-01 (1.9624e-01)	Acc@1   0.44 (  0.44)	Acc@5   0.00 (  0.00)
 * Acc@1 0.441 Acc@5 0.000
28
TRAINING
Epoch: [28][0/1]	Time  0.961 ( 0.961)	Data  0.236 ( 0.236)	Loss 9.3780e-02 (9.3780e-02)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
[0.8293473960740014]
Test: [0/1]	Time  0.250 ( 0.250)	Loss 1.7948e+00 (1.7948e+00)	Acc@1   0.76 (  0.76)	Acc@5   0.00 (  0.00)
 * Acc@1 0.759 Acc@5 0.000
Test: [0/1]	Time  0.319 ( 0.319)	Loss 1.9088e-01 (1.9088e-01)	Acc@1   0.46 (  0.46)	Acc@5   0.00 (  0.00)
 * Acc@1 0.462 Acc@5 0.000
29
TRAINING
Epoch: [29][0/1]	Time  0.962 ( 0.962)	Data  0.227 ( 0.227)	Loss 9.2450e-02 (9.2450e-02)	Acc@1   0.84 (  0.84)	Acc@5   0.00 (  0.00)
[0.8427762252362008]
Test: [0/1]	Time  0.251 ( 0.251)	Loss 1.7932e+00 (1.7932e+00)	Acc@1   0.77 (  0.77)	Acc@5   0.00 (  0.00)
 * Acc@1 0.766 Acc@5 0.000
Test: [0/1]	Time  0.316 ( 0.316)	Loss 1.8602e-01 (1.8602e-01)	Acc@1   0.48 (  0.48)	Acc@5   0.00 (  0.00)
 * Acc@1 0.480 Acc@5 0.000
30
TRAINING
Epoch: [30][0/1]	Time  0.954 ( 0.954)	Data  0.222 ( 0.222)	Loss 9.1969e-02 (9.1969e-02)	Acc@1   0.85 (  0.85)	Acc@5   0.00 (  0.00)
[0.8549858589252446]
Test: [0/1]	Time  0.250 ( 0.250)	Loss 1.7927e+00 (1.7927e+00)	Acc@1   0.77 (  0.77)	Acc@5   0.00 (  0.00)
 * Acc@1 0.771 Acc@5 0.000
Test: [0/1]	Time  0.315 ( 0.315)	Loss 1.8044e-01 (1.8044e-01)	Acc@1   0.50 (  0.50)	Acc@5   0.00 (  0.00)
 * Acc@1 0.500 Acc@5 0.000
31
TRAINING
Epoch: [31][0/1]	Time  0.938 ( 0.938)	Data  0.217 ( 0.217)	Loss 9.0601e-02 (9.0601e-02)	Acc@1   0.86 (  0.86)	Acc@5   0.00 (  0.00)
[0.8638863810772647]
Test: [0/1]	Time  0.247 ( 0.247)	Loss 1.7934e+00 (1.7934e+00)	Acc@1   0.77 (  0.77)	Acc@5   0.00 (  0.00)
 * Acc@1 0.774 Acc@5 0.000
Test: [0/1]	Time  0.313 ( 0.313)	Loss 1.7389e-01 (1.7389e-01)	Acc@1   0.52 (  0.52)	Acc@5   0.00 (  0.00)
 * Acc@1 0.521 Acc@5 0.000
32
TRAINING
Epoch: [32][0/1]	Time  0.999 ( 0.999)	Data  0.274 ( 0.274)	Loss 8.7893e-02 (8.7893e-02)	Acc@1   0.87 (  0.87)	Acc@5   0.00 (  0.00)
[0.8702424476383286]
Test: [0/1]	Time  0.258 ( 0.258)	Loss 1.7949e+00 (1.7949e+00)	Acc@1   0.78 (  0.78)	Acc@5   0.00 (  0.00)
 * Acc@1 0.776 Acc@5 0.000
Test: [0/1]	Time  0.319 ( 0.319)	Loss 1.6704e-01 (1.6704e-01)	Acc@1   0.54 (  0.54)	Acc@5   0.00 (  0.00)
 * Acc@1 0.541 Acc@5 0.000
33
TRAINING
Epoch: [33][0/1]	Time  0.971 ( 0.971)	Data  0.230 ( 0.230)	Loss 8.4582e-02 (8.4582e-02)	Acc@1   0.87 (  0.87)	Acc@5   0.00 (  0.00)
[0.874538147344647]
Test: [0/1]	Time  0.253 ( 0.253)	Loss 1.7972e+00 (1.7972e+00)	Acc@1   0.78 (  0.78)	Acc@5   0.00 (  0.00)
 * Acc@1 0.778 Acc@5 0.000
Test: [0/1]	Time  0.320 ( 0.320)	Loss 1.6096e-01 (1.6096e-01)	Acc@1   0.56 (  0.56)	Acc@5   0.00 (  0.00)
 * Acc@1 0.560 Acc@5 0.000
34
TRAINING
Epoch: [34][0/1]	Time  0.978 ( 0.978)	Data  0.230 ( 0.230)	Loss 8.1880e-02 (8.1880e-02)	Acc@1   0.88 (  0.88)	Acc@5   0.00 (  0.00)
[0.8771236803176345]
Test: [0/1]	Time  0.254 ( 0.254)	Loss 1.8002e+00 (1.8002e+00)	Acc@1   0.78 (  0.78)	Acc@5   0.00 (  0.00)
 * Acc@1 0.778 Acc@5 0.000
Test: [0/1]	Time  0.320 ( 0.320)	Loss 1.5641e-01 (1.5641e-01)	Acc@1   0.58 (  0.58)	Acc@5   0.00 (  0.00)
 * Acc@1 0.579 Acc@5 0.000
35
TRAINING
Epoch: [35][0/1]	Time  0.962 ( 0.962)	Data  0.229 ( 0.229)	Loss 8.0647e-02 (8.0647e-02)	Acc@1   0.88 (  0.88)	Acc@5   0.00 (  0.00)
[0.8783175596729541]
Test: [0/1]	Time  0.258 ( 0.258)	Loss 1.8038e+00 (1.8038e+00)	Acc@1   0.78 (  0.78)	Acc@5   0.00 (  0.00)
 * Acc@1 0.779 Acc@5 0.000
Test: [0/1]	Time  0.323 ( 0.323)	Loss 1.5351e-01 (1.5351e-01)	Acc@1   0.60 (  0.60)	Acc@5   0.00 (  0.00)
 * Acc@1 0.595 Acc@5 0.000
36
TRAINING
Epoch: [36][0/1]	Time  0.949 ( 0.949)	Data  0.224 ( 0.224)	Loss 8.0940e-02 (8.0940e-02)	Acc@1   0.88 (  0.88)	Acc@5   0.00 (  0.00)
[0.8783894433807742]
Test: [0/1]	Time  0.254 ( 0.254)	Loss 1.8078e+00 (1.8078e+00)	Acc@1   0.78 (  0.78)	Acc@5   0.00 (  0.00)
 * Acc@1 0.781 Acc@5 0.000
Test: [0/1]	Time  0.322 ( 0.322)	Loss 1.5177e-01 (1.5177e-01)	Acc@1   0.61 (  0.61)	Acc@5   0.00 (  0.00)
 * Acc@1 0.613 Acc@5 0.000
37
TRAINING
Epoch: [37][0/1]	Time  0.966 ( 0.966)	Data  0.232 ( 0.232)	Loss 8.2099e-02 (8.2099e-02)	Acc@1   0.88 (  0.88)	Acc@5   0.00 (  0.00)
[0.877043538115083]
Test: [0/1]	Time  0.252 ( 0.252)	Loss 1.8123e+00 (1.8123e+00)	Acc@1   0.78 (  0.78)	Acc@5   0.00 (  0.00)
 * Acc@1 0.781 Acc@5 0.000
Test: [0/1]	Time  0.313 ( 0.313)	Loss 1.5045e-01 (1.5045e-01)	Acc@1   0.63 (  0.63)	Acc@5   0.00 (  0.00)
 * Acc@1 0.628 Acc@5 0.000
38
TRAINING
Epoch: [38][0/1]	Time  0.945 ( 0.945)	Data  0.222 ( 0.222)	Loss 8.3229e-02 (8.3229e-02)	Acc@1   0.88 (  0.88)	Acc@5   0.00 (  0.00)
[0.8772483138038012]
Test: [0/1]	Time  0.249 ( 0.249)	Loss 1.8172e+00 (1.8172e+00)	Acc@1   0.78 (  0.78)	Acc@5   0.00 (  0.00)
 * Acc@1 0.785 Acc@5 0.000
Test: [0/1]	Time  0.317 ( 0.317)	Loss 1.4905e-01 (1.4905e-01)	Acc@1   0.64 (  0.64)	Acc@5   0.00 (  0.00)
 * Acc@1 0.644 Acc@5 0.000
39
TRAINING
Epoch: [39][0/1]	Time  0.948 ( 0.948)	Data  0.218 ( 0.218)	Loss 8.3720e-02 (8.3720e-02)	Acc@1   0.88 (  0.88)	Acc@5   0.00 (  0.00)
[0.8776781224754913]
Test: [0/1]	Time  0.257 ( 0.257)	Loss 1.8222e+00 (1.8222e+00)	Acc@1   0.79 (  0.79)	Acc@5   0.00 (  0.00)
 * Acc@1 0.788 Acc@5 0.000
Test: [0/1]	Time  0.316 ( 0.316)	Loss 1.4747e-01 (1.4747e-01)	Acc@1   0.66 (  0.66)	Acc@5   0.00 (  0.00)
 * Acc@1 0.658 Acc@5 0.000
40
TRAINING
Epoch: [40][0/1]	Time  0.971 ( 0.971)	Data  0.223 ( 0.223)	Loss 8.3515e-02 (8.3515e-02)	Acc@1   0.88 (  0.88)	Acc@5   0.00 (  0.00)
[0.8780709001212762]
Test: [0/1]	Time  0.249 ( 0.249)	Loss 1.8272e+00 (1.8272e+00)	Acc@1   0.79 (  0.79)	Acc@5   0.00 (  0.00)
 * Acc@1 0.792 Acc@5 0.000
Test: [0/1]	Time  0.317 ( 0.317)	Loss 1.4598e-01 (1.4598e-01)	Acc@1   0.67 (  0.67)	Acc@5   0.00 (  0.00)
 * Acc@1 0.669 Acc@5 0.000
41
TRAINING
Epoch: [41][0/1]	Time  0.963 ( 0.963)	Data  0.224 ( 0.224)	Loss 8.3009e-02 (8.3009e-02)	Acc@1   0.88 (  0.88)	Acc@5   0.00 (  0.00)
[0.8784485049533793]
Test: [0/1]	Time  0.250 ( 0.250)	Loss 1.8322e+00 (1.8322e+00)	Acc@1   0.80 (  0.80)	Acc@5   0.00 (  0.00)
 * Acc@1 0.795 Acc@5 0.000
Test: [0/1]	Time  0.316 ( 0.316)	Loss 1.4498e-01 (1.4498e-01)	Acc@1   0.68 (  0.68)	Acc@5   0.00 (  0.00)
 * Acc@1 0.677 Acc@5 0.000
42
TRAINING
Epoch: [42][0/1]	Time  0.959 ( 0.959)	Data  0.223 ( 0.223)	Loss 8.2724e-02 (8.2724e-02)	Acc@1   0.88 (  0.88)	Acc@5   0.00 (  0.00)
[0.8801833646258181]
Test: [0/1]	Time  0.251 ( 0.251)	Loss 1.8367e+00 (1.8367e+00)	Acc@1   0.80 (  0.80)	Acc@5   0.00 (  0.00)
 * Acc@1 0.798 Acc@5 0.000
Test: [0/1]	Time  0.316 ( 0.316)	Loss 1.4469e-01 (1.4469e-01)	Acc@1   0.68 (  0.68)	Acc@5   0.00 (  0.00)
 * Acc@1 0.681 Acc@5 0.000
43
TRAINING
Epoch: [43][0/1]	Time  0.973 ( 0.973)	Data  0.225 ( 0.225)	Loss 8.2966e-02 (8.2966e-02)	Acc@1   0.88 (  0.88)	Acc@5   0.00 (  0.00)
[0.8815209597067099]
Test: [0/1]	Time  0.252 ( 0.252)	Loss 1.8408e+00 (1.8408e+00)	Acc@1   0.80 (  0.80)	Acc@5   0.00 (  0.00)
 * Acc@1 0.801 Acc@5 0.000
Test: [0/1]	Time  0.315 ( 0.315)	Loss 1.4500e-01 (1.4500e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.686 Acc@5 0.000
44
TRAINING
Epoch: [44][0/1]	Time  0.957 ( 0.957)	Data  0.225 ( 0.225)	Loss 8.3677e-02 (8.3677e-02)	Acc@1   0.88 (  0.88)	Acc@5   0.00 (  0.00)
[0.8821227277838251]
Test: [0/1]	Time  0.250 ( 0.250)	Loss 1.8442e+00 (1.8442e+00)	Acc@1   0.80 (  0.80)	Acc@5   0.00 (  0.00)
 * Acc@1 0.804 Acc@5 0.000
Test: [0/1]	Time  0.314 ( 0.314)	Loss 1.4556e-01 (1.4556e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.689 Acc@5 0.000
45
TRAINING
Epoch: [45][0/1]	Time  0.958 ( 0.958)	Data  0.226 ( 0.226)	Loss 8.4517e-02 (8.4517e-02)	Acc@1   0.88 (  0.88)	Acc@5   0.00 (  0.00)
[0.8829482938403306]
Test: [0/1]	Time  0.251 ( 0.251)	Loss 1.8469e+00 (1.8469e+00)	Acc@1   0.81 (  0.81)	Acc@5   0.00 (  0.00)
 * Acc@1 0.807 Acc@5 0.000
Test: [0/1]	Time  0.317 ( 0.317)	Loss 1.4598e-01 (1.4598e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.691 Acc@5 0.000
46
TRAINING
Epoch: [46][0/1]	Time  0.957 ( 0.957)	Data  0.228 ( 0.228)	Loss 8.5097e-02 (8.5097e-02)	Acc@1   0.88 (  0.88)	Acc@5   0.00 (  0.00)
[0.8841416452570947]
Test: [0/1]	Time  0.252 ( 0.252)	Loss 1.8489e+00 (1.8489e+00)	Acc@1   0.81 (  0.81)	Acc@5   0.00 (  0.00)
 * Acc@1 0.810 Acc@5 0.000
Test: [0/1]	Time  0.316 ( 0.316)	Loss 1.4602e-01 (1.4602e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.693 Acc@5 0.000
47
TRAINING
Epoch: [47][0/1]	Time  0.970 ( 0.970)	Data  0.229 ( 0.229)	Loss 8.5195e-02 (8.5195e-02)	Acc@1   0.89 (  0.89)	Acc@5   0.00 (  0.00)
[0.8857778015406408]
Test: [0/1]	Time  0.252 ( 0.252)	Loss 1.8500e+00 (1.8500e+00)	Acc@1   0.81 (  0.81)	Acc@5   0.00 (  0.00)
 * Acc@1 0.813 Acc@5 0.000
Test: [0/1]	Time  0.315 ( 0.315)	Loss 1.4568e-01 (1.4568e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.694 Acc@5 0.000
48
TRAINING
Epoch: [48][0/1]	Time  0.946 ( 0.946)	Data  0.225 ( 0.225)	Loss 8.4849e-02 (8.4849e-02)	Acc@1   0.89 (  0.89)	Acc@5   0.00 (  0.00)
[0.8878343391327201]
Test: [0/1]	Time  0.248 ( 0.248)	Loss 1.8505e+00 (1.8505e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.815 Acc@5 0.000
Test: [0/1]	Time  0.315 ( 0.315)	Loss 1.4517e-01 (1.4517e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.694 Acc@5 0.000
49
TRAINING
Epoch: [49][0/1]	Time  0.957 ( 0.957)	Data  0.227 ( 0.227)	Loss 8.4288e-02 (8.4288e-02)	Acc@1   0.89 (  0.89)	Acc@5   0.00 (  0.00)
[0.8916431450464248]
Test: [0/1]	Time  0.250 ( 0.250)	Loss 1.8503e+00 (1.8503e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.818 Acc@5 0.000
Test: [0/1]	Time  0.313 ( 0.313)	Loss 1.4471e-01 (1.4471e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.694 Acc@5 0.000
50
TRAINING
Epoch: [50][0/1]	Time  0.958 ( 0.958)	Data  0.219 ( 0.219)	Loss 8.3777e-02 (8.3777e-02)	Acc@1   0.90 (  0.90)	Acc@5   0.00 (  0.00)
[0.8959512971934714]
Test: [0/1]	Time  0.248 ( 0.248)	Loss 1.8495e+00 (1.8495e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.820 Acc@5 0.000
Test: [0/1]	Time  0.314 ( 0.314)	Loss 1.4446e-01 (1.4446e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.694 Acc@5 0.000
51
TRAINING
Epoch: [51][0/1]	Time  0.933 ( 0.933)	Data  0.225 ( 0.225)	Loss 8.3472e-02 (8.3472e-02)	Acc@1   0.90 (  0.90)	Acc@5   0.00 (  0.00)
[0.9011962540684753]
Test: [0/1]	Time  0.255 ( 0.255)	Loss 1.8483e+00 (1.8483e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.822 Acc@5 0.000
Test: [0/1]	Time  0.322 ( 0.322)	Loss 1.4441e-01 (1.4441e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.693 Acc@5 0.000
52
TRAINING
Epoch: [52][0/1]	Time  0.965 ( 0.965)	Data  0.257 ( 0.257)	Loss 8.3365e-02 (8.3365e-02)	Acc@1   0.91 (  0.91)	Acc@5   0.00 (  0.00)
[0.9060950309492535]
Test: [0/1]	Time  0.253 ( 0.253)	Loss 1.8469e+00 (1.8469e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.823 Acc@5 0.000
Test: [0/1]	Time  0.320 ( 0.320)	Loss 1.4448e-01 (1.4448e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.693 Acc@5 0.000
53
TRAINING
Epoch: [53][0/1]	Time  0.940 ( 0.940)	Data  0.232 ( 0.232)	Loss 8.3329e-02 (8.3329e-02)	Acc@1   0.91 (  0.91)	Acc@5   0.00 (  0.00)
[0.9103466065426713]
Test: [0/1]	Time  0.254 ( 0.254)	Loss 1.8453e+00 (1.8453e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.824 Acc@5 0.000
Test: [0/1]	Time  0.321 ( 0.321)	Loss 1.4453e-01 (1.4453e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.695 Acc@5 0.000
54
TRAINING
Epoch: [54][0/1]	Time  0.960 ( 0.960)	Data  0.227 ( 0.227)	Loss 8.3226e-02 (8.3226e-02)	Acc@1   0.91 (  0.91)	Acc@5   0.00 (  0.00)
[0.9139313475675319]
Test: [0/1]	Time  0.254 ( 0.254)	Loss 1.8435e+00 (1.8435e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.825 Acc@5 0.000
Test: [0/1]	Time  0.317 ( 0.317)	Loss 1.4449e-01 (1.4449e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.694 Acc@5 0.000
55
TRAINING
Epoch: [55][0/1]	Time  0.945 ( 0.945)	Data  0.236 ( 0.236)	Loss 8.2987e-02 (8.2987e-02)	Acc@1   0.92 (  0.92)	Acc@5   0.00 (  0.00)
[0.9170796277332803]
Test: [0/1]	Time  0.255 ( 0.255)	Loss 1.8415e+00 (1.8415e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.826 Acc@5 0.000
Test: [0/1]	Time  0.317 ( 0.317)	Loss 1.4439e-01 (1.4439e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.693 Acc@5 0.000
56
TRAINING
Epoch: [56][0/1]	Time  0.945 ( 0.945)	Data  0.234 ( 0.234)	Loss 8.2645e-02 (8.2645e-02)	Acc@1   0.92 (  0.92)	Acc@5   0.00 (  0.00)
[0.9196502210000215]
Test: [0/1]	Time  0.256 ( 0.256)	Loss 1.8396e+00 (1.8396e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.827 Acc@5 0.000
Test: [0/1]	Time  0.321 ( 0.321)	Loss 1.4428e-01 (1.4428e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.692 Acc@5 0.000
57
TRAINING
Epoch: [57][0/1]	Time  0.938 ( 0.938)	Data  0.230 ( 0.230)	Loss 8.2300e-02 (8.2300e-02)	Acc@1   0.92 (  0.92)	Acc@5   0.00 (  0.00)
[0.9215507977541837]
Test: [0/1]	Time  0.253 ( 0.253)	Loss 1.8376e+00 (1.8376e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.827 Acc@5 0.000
Test: [0/1]	Time  0.319 ( 0.319)	Loss 1.4424e-01 (1.4424e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.691 Acc@5 0.000
58
TRAINING
Epoch: [58][0/1]	Time  0.944 ( 0.944)	Data  0.233 ( 0.233)	Loss 8.2045e-02 (8.2045e-02)	Acc@1   0.92 (  0.92)	Acc@5   0.00 (  0.00)
[0.9238797559298151]
Test: [0/1]	Time  0.257 ( 0.257)	Loss 1.8356e+00 (1.8356e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.828 Acc@5 0.000
Test: [0/1]	Time  0.319 ( 0.319)	Loss 1.4429e-01 (1.4429e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.693 Acc@5 0.000
59
TRAINING
Epoch: [59][0/1]	Time  0.942 ( 0.942)	Data  0.231 ( 0.231)	Loss 8.1918e-02 (8.1918e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9264615071939686]
Test: [0/1]	Time  0.254 ( 0.254)	Loss 1.8336e+00 (1.8336e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.828 Acc@5 0.000
Test: [0/1]	Time  0.321 ( 0.321)	Loss 1.4437e-01 (1.4437e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.692 Acc@5 0.000
60
TRAINING
Epoch: [60][0/1]	Time  0.957 ( 0.957)	Data  0.223 ( 0.223)	Loss 8.1882e-02 (8.1882e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9287606221057381]
Test: [0/1]	Time  0.252 ( 0.252)	Loss 1.8317e+00 (1.8317e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.828 Acc@5 0.000
Test: [0/1]	Time  0.320 ( 0.320)	Loss 1.4441e-01 (1.4441e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.692 Acc@5 0.000
61
TRAINING
Epoch: [61][0/1]	Time  0.941 ( 0.941)	Data  0.233 ( 0.233)	Loss 8.1862e-02 (8.1862e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9307845560102531]
Test: [0/1]	Time  0.253 ( 0.253)	Loss 1.8298e+00 (1.8298e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.828 Acc@5 0.000
Test: [0/1]	Time  0.319 ( 0.319)	Loss 1.4434e-01 (1.4434e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.691 Acc@5 0.000
62
TRAINING
Epoch: [62][0/1]	Time  0.949 ( 0.949)	Data  0.238 ( 0.238)	Loss 8.1788e-02 (8.1788e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9325468094006686]
Test: [0/1]	Time  0.257 ( 0.257)	Loss 1.8279e+00 (1.8279e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.828 Acc@5 0.000
Test: [0/1]	Time  0.318 ( 0.318)	Loss 1.4413e-01 (1.4413e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.691 Acc@5 0.000
63
TRAINING
Epoch: [63][0/1]	Time  0.953 ( 0.953)	Data  0.234 ( 0.234)	Loss 8.1631e-02 (8.1631e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9338659862629124]
Test: [0/1]	Time  0.253 ( 0.253)	Loss 1.8262e+00 (1.8262e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.828 Acc@5 0.000
Test: [0/1]	Time  0.320 ( 0.320)	Loss 1.4383e-01 (1.4383e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.692 Acc@5 0.000
64
TRAINING
Epoch: [64][0/1]	Time  0.942 ( 0.942)	Data  0.233 ( 0.233)	Loss 8.1414e-02 (8.1414e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9347762709549677]
Test: [0/1]	Time  0.253 ( 0.253)	Loss 1.8247e+00 (1.8247e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.828 Acc@5 0.000
Test: [0/1]	Time  0.316 ( 0.316)	Loss 1.4347e-01 (1.4347e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.693 Acc@5 0.000
65
TRAINING
Epoch: [65][0/1]	Time  0.939 ( 0.939)	Data  0.231 ( 0.231)	Loss 8.1187e-02 (8.1187e-02)	Acc@1   0.94 (  0.94)	Acc@5   0.00 (  0.00)
[0.9355548854476594]
Test: [0/1]	Time  0.259 ( 0.259)	Loss 1.8233e+00 (1.8233e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.828 Acc@5 0.000
Test: [0/1]	Time  0.321 ( 0.321)	Loss 1.4313e-01 (1.4313e-01)	Acc@1   0.69 (  0.69)	Acc@5   0.00 (  0.00)
 * Acc@1 0.694 Acc@5 0.000
66
TRAINING
Epoch: [66][0/1]	Time  0.943 ( 0.943)	Data  0.233 ( 0.233)	Loss 8.0997e-02 (8.0997e-02)	Acc@1   0.94 (  0.94)	Acc@5   0.00 (  0.00)
[0.9360277315430878]
Test: [0/1]	Time  0.261 ( 0.261)	Loss 1.8221e+00 (1.8221e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.828 Acc@5 0.000
Test: [0/1]	Time  0.327 ( 0.327)	Loss 1.4285e-01 (1.4285e-01)	Acc@1   0.70 (  0.70)	Acc@5   0.00 (  0.00)
 * Acc@1 0.695 Acc@5 0.000
67
TRAINING
Epoch: [67][0/1]	Time  0.965 ( 0.965)	Data  0.231 ( 0.231)	Loss 8.0864e-02 (8.0864e-02)	Acc@1   0.94 (  0.94)	Acc@5   0.00 (  0.00)
[0.9361341022519367]
Test: [0/1]	Time  0.254 ( 0.254)	Loss 1.8211e+00 (1.8211e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.827 Acc@5 0.000
Test: [0/1]	Time  0.317 ( 0.317)	Loss 1.4261e-01 (1.4261e-01)	Acc@1   0.70 (  0.70)	Acc@5   0.00 (  0.00)
 * Acc@1 0.696 Acc@5 0.000
68
TRAINING
Epoch: [68][0/1]	Time  0.972 ( 0.972)	Data  0.224 ( 0.224)	Loss 8.0780e-02 (8.0780e-02)	Acc@1   0.94 (  0.94)	Acc@5   0.00 (  0.00)
[0.9361291284742621]
Test: [0/1]	Time  0.262 ( 0.262)	Loss 1.8204e+00 (1.8204e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.827 Acc@5 0.000
Test: [0/1]	Time  0.318 ( 0.318)	Loss 1.4241e-01 (1.4241e-01)	Acc@1   0.70 (  0.70)	Acc@5   0.00 (  0.00)
 * Acc@1 0.697 Acc@5 0.000
69
TRAINING
Epoch: [69][0/1]	Time  0.966 ( 0.966)	Data  0.237 ( 0.237)	Loss 8.0717e-02 (8.0717e-02)	Acc@1   0.94 (  0.94)	Acc@5   0.00 (  0.00)
[0.936099700742528]
Test: [0/1]	Time  0.254 ( 0.254)	Loss 1.8199e+00 (1.8199e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.827 Acc@5 0.000
Test: [0/1]	Time  0.321 ( 0.321)	Loss 1.4222e-01 (1.4222e-01)	Acc@1   0.70 (  0.70)	Acc@5   0.00 (  0.00)
 * Acc@1 0.698 Acc@5 0.000
70
TRAINING
Epoch: [70][0/1]	Time  0.955 ( 0.955)	Data  0.223 ( 0.223)	Loss 8.0651e-02 (8.0651e-02)	Acc@1   0.94 (  0.94)	Acc@5   0.00 (  0.00)
[0.9359980556725366]
Test: [0/1]	Time  0.249 ( 0.249)	Loss 1.8196e+00 (1.8196e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.826 Acc@5 0.000
Test: [0/1]	Time  0.321 ( 0.321)	Loss 1.4204e-01 (1.4204e-01)	Acc@1   0.70 (  0.70)	Acc@5   0.00 (  0.00)
 * Acc@1 0.699 Acc@5 0.000
71
TRAINING
Epoch: [71][0/1]	Time  0.973 ( 0.973)	Data  0.224 ( 0.224)	Loss 8.0574e-02 (8.0574e-02)	Acc@1   0.94 (  0.94)	Acc@5   0.00 (  0.00)
[0.935824739027832]
Test: [0/1]	Time  0.252 ( 0.252)	Loss 1.8194e+00 (1.8194e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.826 Acc@5 0.000
Test: [0/1]	Time  0.315 ( 0.315)	Loss 1.4187e-01 (1.4187e-01)	Acc@1   0.70 (  0.70)	Acc@5   0.00 (  0.00)
 * Acc@1 0.700 Acc@5 0.000
72
TRAINING
Epoch: [72][0/1]	Time  0.958 ( 0.958)	Data  0.225 ( 0.225)	Loss 8.0495e-02 (8.0495e-02)	Acc@1   0.94 (  0.94)	Acc@5   0.00 (  0.00)
[0.9355976068279874]
Test: [0/1]	Time  0.250 ( 0.250)	Loss 1.8194e+00 (1.8194e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.826 Acc@5 0.000
Test: [0/1]	Time  0.317 ( 0.317)	Loss 1.4172e-01 (1.4172e-01)	Acc@1   0.70 (  0.70)	Acc@5   0.00 (  0.00)
 * Acc@1 0.700 Acc@5 0.000
73
TRAINING
Epoch: [73][0/1]	Time  0.972 ( 0.972)	Data  0.225 ( 0.225)	Loss 8.0432e-02 (8.0432e-02)	Acc@1   0.94 (  0.94)	Acc@5   0.00 (  0.00)
[0.9353330526362149]
Test: [0/1]	Time  0.250 ( 0.250)	Loss 1.8195e+00 (1.8195e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.825 Acc@5 0.000
Test: [0/1]	Time  0.315 ( 0.315)	Loss 1.4161e-01 (1.4161e-01)	Acc@1   0.70 (  0.70)	Acc@5   0.00 (  0.00)
 * Acc@1 0.701 Acc@5 0.000
74
TRAINING
Epoch: [74][0/1]	Time  0.965 ( 0.965)	Data  0.227 ( 0.227)	Loss 8.0397e-02 (8.0397e-02)	Acc@1   0.94 (  0.94)	Acc@5   0.00 (  0.00)
[0.9350469117550428]
Test: [0/1]	Time  0.251 ( 0.251)	Loss 1.8196e+00 (1.8196e+00)	Acc@1   0.83 (  0.83)	Acc@5   0.00 (  0.00)
 * Acc@1 0.825 Acc@5 0.000
Test: [0/1]	Time  0.317 ( 0.317)	Loss 1.4152e-01 (1.4152e-01)	Acc@1   0.70 (  0.70)	Acc@5   0.00 (  0.00)
 * Acc@1 0.701 Acc@5 0.000
75
TRAINING
Epoch: [75][0/1]	Time  0.950 ( 0.950)	Data  0.227 ( 0.227)	Loss 8.0390e-02 (8.0390e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9347889161052596]
Test: [0/1]	Time  0.255 ( 0.255)	Loss 1.8198e+00 (1.8198e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.825 Acc@5 0.000
Test: [0/1]	Time  0.321 ( 0.321)	Loss 1.4145e-01 (1.4145e-01)	Acc@1   0.70 (  0.70)	Acc@5   0.00 (  0.00)
 * Acc@1 0.701 Acc@5 0.000
76
TRAINING
Epoch: [76][0/1]	Time  0.972 ( 0.972)	Data  0.228 ( 0.228)	Loss 8.0397e-02 (8.0397e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9346330462272596]
Test: [0/1]	Time  0.249 ( 0.249)	Loss 1.8199e+00 (1.8199e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.825 Acc@5 0.000
Test: [0/1]	Time  0.315 ( 0.315)	Loss 1.4137e-01 (1.4137e-01)	Acc@1   0.70 (  0.70)	Acc@5   0.00 (  0.00)
 * Acc@1 0.701 Acc@5 0.000
77
TRAINING
Epoch: [77][0/1]	Time  0.950 ( 0.950)	Data  0.224 ( 0.224)	Loss 8.0401e-02 (8.0401e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9344876677375957]
Test: [0/1]	Time  0.251 ( 0.251)	Loss 1.8200e+00 (1.8200e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.824 Acc@5 0.000
Test: [0/1]	Time  0.315 ( 0.315)	Loss 1.4127e-01 (1.4127e-01)	Acc@1   0.70 (  0.70)	Acc@5   0.00 (  0.00)
 * Acc@1 0.702 Acc@5 0.000
78
TRAINING
Epoch: [78][0/1]	Time  0.960 ( 0.960)	Data  0.227 ( 0.227)	Loss 8.0391e-02 (8.0391e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9343559979077326]
Test: [0/1]	Time  0.249 ( 0.249)	Loss 1.8201e+00 (1.8201e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.824 Acc@5 0.000
Test: [0/1]	Time  0.315 ( 0.315)	Loss 1.4116e-01 (1.4116e-01)	Acc@1   0.70 (  0.70)	Acc@5   0.00 (  0.00)
 * Acc@1 0.702 Acc@5 0.000
79
TRAINING
Epoch: [79][0/1]	Time  0.956 ( 0.956)	Data  0.223 ( 0.223)	Loss 8.0363e-02 (8.0363e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9342403949090987]
Test: [0/1]	Time  0.254 ( 0.254)	Loss 1.8201e+00 (1.8201e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.824 Acc@5 0.000
Test: [0/1]	Time  0.316 ( 0.316)	Loss 1.4103e-01 (1.4103e-01)	Acc@1   0.70 (  0.70)	Acc@5   0.00 (  0.00)
 * Acc@1 0.702 Acc@5 0.000
80
TRAINING
Epoch: [80][0/1]	Time  0.967 ( 0.967)	Data  0.233 ( 0.233)	Loss 8.0325e-02 (8.0325e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9341423673915814]
Test: [0/1]	Time  0.252 ( 0.252)	Loss 1.8201e+00 (1.8201e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.824 Acc@5 0.000
Test: [0/1]	Time  0.312 ( 0.312)	Loss 1.4092e-01 (1.4092e-01)	Acc@1   0.70 (  0.70)	Acc@5   0.00 (  0.00)
 * Acc@1 0.703 Acc@5 0.000
81
TRAINING
Epoch: [81][0/1]	Time  0.948 ( 0.948)	Data  0.221 ( 0.221)	Loss 8.0288e-02 (8.0288e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9340635883761121]
Test: [0/1]	Time  0.258 ( 0.258)	Loss 1.8200e+00 (1.8200e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.824 Acc@5 0.000
Test: [0/1]	Time  0.318 ( 0.318)	Loss 1.4081e-01 (1.4081e-01)	Acc@1   0.70 (  0.70)	Acc@5   0.00 (  0.00)
 * Acc@1 0.704 Acc@5 0.000
82
TRAINING
Epoch: [82][0/1]	Time  0.960 ( 0.960)	Data  0.228 ( 0.228)	Loss 8.0260e-02 (8.0260e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9340069184156781]
Test: [0/1]	Time  0.251 ( 0.251)	Loss 1.8200e+00 (1.8200e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.824 Acc@5 0.000
Test: [0/1]	Time  0.317 ( 0.317)	Loss 1.4073e-01 (1.4073e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.705 Acc@5 0.000
83
TRAINING
Epoch: [83][0/1]	Time  0.978 ( 0.978)	Data  0.230 ( 0.230)	Loss 8.0245e-02 (8.0245e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9339267520416477]
Test: [0/1]	Time  0.252 ( 0.252)	Loss 1.8199e+00 (1.8199e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.823 Acc@5 0.000
Test: [0/1]	Time  0.316 ( 0.316)	Loss 1.4067e-01 (1.4067e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.706 Acc@5 0.000
84
TRAINING
Epoch: [84][0/1]	Time  0.963 ( 0.963)	Data  0.230 ( 0.230)	Loss 8.0239e-02 (8.0239e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9338559682075088]
Test: [0/1]	Time  0.253 ( 0.253)	Loss 1.8199e+00 (1.8199e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.823 Acc@5 0.000
Test: [0/1]	Time  0.316 ( 0.316)	Loss 1.4063e-01 (1.4063e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.707 Acc@5 0.000
85
TRAINING
Epoch: [85][0/1]	Time  0.962 ( 0.962)	Data  0.229 ( 0.229)	Loss 8.0238e-02 (8.0238e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9339353203068306]
Test: [0/1]	Time  0.251 ( 0.251)	Loss 1.8198e+00 (1.8198e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.823 Acc@5 0.000
Test: [0/1]	Time  0.315 ( 0.315)	Loss 1.4060e-01 (1.4060e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.708 Acc@5 0.000
86
TRAINING
Epoch: [86][0/1]	Time  0.970 ( 0.970)	Data  0.226 ( 0.226)	Loss 8.0238e-02 (8.0238e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9340730236133512]
Test: [0/1]	Time  0.252 ( 0.252)	Loss 1.8197e+00 (1.8197e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.823 Acc@5 0.000
Test: [0/1]	Time  0.316 ( 0.316)	Loss 1.4058e-01 (1.4058e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.709 Acc@5 0.000
87
TRAINING
Epoch: [87][0/1]	Time  0.971 ( 0.971)	Data  0.226 ( 0.226)	Loss 8.0238e-02 (8.0238e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9342511368126323]
Test: [0/1]	Time  0.257 ( 0.257)	Loss 1.8197e+00 (1.8197e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.823 Acc@5 0.000
Test: [0/1]	Time  0.316 ( 0.316)	Loss 1.4058e-01 (1.4058e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.709 Acc@5 0.000
88
TRAINING
Epoch: [88][0/1]	Time  0.974 ( 0.974)	Data  0.233 ( 0.233)	Loss 8.0239e-02 (8.0239e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9344549400337947]
Test: [0/1]	Time  0.256 ( 0.256)	Loss 1.8196e+00 (1.8196e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.823 Acc@5 0.000
Test: [0/1]	Time  0.321 ( 0.321)	Loss 1.4058e-01 (1.4058e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.709 Acc@5 0.000
89
TRAINING
Epoch: [89][0/1]	Time  0.956 ( 0.956)	Data  0.231 ( 0.231)	Loss 8.0244e-02 (8.0244e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9346080532099494]
Test: [0/1]	Time  0.256 ( 0.256)	Loss 1.8196e+00 (1.8196e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.823 Acc@5 0.000
Test: [0/1]	Time  0.316 ( 0.316)	Loss 1.4059e-01 (1.4059e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.709 Acc@5 0.000
90
TRAINING
Epoch: [90][0/1]	Time  0.957 ( 0.957)	Data  0.221 ( 0.221)	Loss 8.0253e-02 (8.0253e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.934776595823168]
Test: [0/1]	Time  0.252 ( 0.252)	Loss 1.8195e+00 (1.8195e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.823 Acc@5 0.000
Test: [0/1]	Time  0.313 ( 0.313)	Loss 1.4061e-01 (1.4061e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.709 Acc@5 0.000
91
TRAINING
Epoch: [91][0/1]	Time  0.958 ( 0.958)	Data  0.227 ( 0.227)	Loss 8.0263e-02 (8.0263e-02)	Acc@1   0.93 (  0.93)	Acc@5   0.00 (  0.00)
[0.9349523131272737]
Test: [0/1]	Time  0.273 ( 0.273)	Loss 1.8194e+00 (1.8194e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.823 Acc@5 0.000
Test: [0/1]	Time  0.324 ( 0.324)	Loss 1.4062e-01 (1.4062e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.709 Acc@5 0.000
92
TRAINING
Epoch: [92][0/1]	Time  0.950 ( 0.950)	Data  0.228 ( 0.228)	Loss 8.0272e-02 (8.0272e-02)	Acc@1   0.94 (  0.94)	Acc@5   0.00 (  0.00)
[0.9351283000230316]
Test: [0/1]	Time  0.251 ( 0.251)	Loss 1.8193e+00 (1.8193e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.823 Acc@5 0.000
Test: [0/1]	Time  0.321 ( 0.321)	Loss 1.4063e-01 (1.4063e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.710 Acc@5 0.000
93
TRAINING
Epoch: [93][0/1]	Time  0.965 ( 0.965)	Data  0.238 ( 0.238)	Loss 8.0275e-02 (8.0275e-02)	Acc@1   0.94 (  0.94)	Acc@5   0.00 (  0.00)
[0.9352992934024139]
Test: [0/1]	Time  0.251 ( 0.251)	Loss 1.8193e+00 (1.8193e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.823 Acc@5 0.000
Test: [0/1]	Time  0.320 ( 0.320)	Loss 1.4063e-01 (1.4063e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.710 Acc@5 0.000
94
TRAINING
Epoch: [94][0/1]	Time  0.977 ( 0.977)	Data  0.234 ( 0.234)	Loss 8.0271e-02 (8.0271e-02)	Acc@1   0.94 (  0.94)	Acc@5   0.00 (  0.00)
[0.9354617218801606]
Test: [0/1]	Time  0.252 ( 0.252)	Loss 1.8192e+00 (1.8192e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.823 Acc@5 0.000
Test: [0/1]	Time  0.316 ( 0.316)	Loss 1.4062e-01 (1.4062e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.710 Acc@5 0.000
95
TRAINING
Epoch: [95][0/1]	Time  0.973 ( 0.973)	Data  0.223 ( 0.223)	Loss 8.0260e-02 (8.0260e-02)	Acc@1   0.94 (  0.94)	Acc@5   0.00 (  0.00)
[0.9356136397390344]
Test: [0/1]	Time  0.252 ( 0.252)	Loss 1.8191e+00 (1.8191e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.823 Acc@5 0.000
Test: [0/1]	Time  0.317 ( 0.317)	Loss 1.4061e-01 (1.4061e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.710 Acc@5 0.000
96
TRAINING
Epoch: [96][0/1]	Time  0.950 ( 0.950)	Data  0.229 ( 0.229)	Loss 8.0244e-02 (8.0244e-02)	Acc@1   0.94 (  0.94)	Acc@5   0.00 (  0.00)
[0.9357545949349328]
Test: [0/1]	Time  0.250 ( 0.250)	Loss 1.8190e+00 (1.8190e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.824 Acc@5 0.000
Test: [0/1]	Time  0.326 ( 0.326)	Loss 1.4059e-01 (1.4059e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.710 Acc@5 0.000
97
TRAINING
Epoch: [97][0/1]	Time  0.968 ( 0.968)	Data  0.232 ( 0.232)	Loss 8.0226e-02 (8.0226e-02)	Acc@1   0.94 (  0.94)	Acc@5   0.00 (  0.00)
[0.9358854007846588]
Test: [0/1]	Time  0.253 ( 0.253)	Loss 1.8189e+00 (1.8189e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.823 Acc@5 0.000
Test: [0/1]	Time  0.318 ( 0.318)	Loss 1.4058e-01 (1.4058e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.710 Acc@5 0.000
98
TRAINING
Epoch: [98][0/1]	Time  0.957 ( 0.957)	Data  0.230 ( 0.230)	Loss 8.0209e-02 (8.0209e-02)	Acc@1   0.94 (  0.94)	Acc@5   0.00 (  0.00)
[0.9360077604796007]
Test: [0/1]	Time  0.249 ( 0.249)	Loss 1.8188e+00 (1.8188e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.823 Acc@5 0.000
Test: [0/1]	Time  0.313 ( 0.313)	Loss 1.4057e-01 (1.4057e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.710 Acc@5 0.000
99
TRAINING
Epoch: [99][0/1]	Time  0.966 ( 0.966)	Data  0.223 ( 0.223)	Loss 8.0193e-02 (8.0193e-02)	Acc@1   0.94 (  0.94)	Acc@5   0.00 (  0.00)
[0.936123744229532]
Test: [0/1]	Time  0.251 ( 0.251)	Loss 1.8188e+00 (1.8188e+00)	Acc@1   0.82 (  0.82)	Acc@5   0.00 (  0.00)
 * Acc@1 0.823 Acc@5 0.000
Test: [0/1]	Time  0.317 ( 0.317)	Loss 1.4056e-01 (1.4056e-01)	Acc@1   0.71 (  0.71)	Acc@5   0.00 (  0.00)
 * Acc@1 0.710 Acc@5 0.000
=> using pre-trained model 'st_resnet'
s/stretch/session_1
torch.Size([640, 3, 224, 224])
(640, 22)
attach reg
0
TRAINING
/home/helenr6/Adv_mapper/train_split.py:125: UserWarning: You have chosen to seed training. This will turn on the CUDNN deterministic setting, which can slow down your training considerably! You may see unexpected behavior when restarting from checkpoints.
  warnings.warn('You have chosen to seed training. '
/localscratch/helenr6.6279148.0/env/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
Epoch: [0][0/1]	Time  5.333 ( 5.333)	Data  1.843 ( 1.843)	Loss 1.7142e+00 (1.7142e+00)	Acc@1  -0.01 ( -0.01)	Acc@5   0.00 (  0.00)
[-0.005528608110923746]
Test: [0/1]	Time  0.435 ( 0.435)	Loss 2.4681e+00 (2.4681e+00)	Acc@1   0.14 (  0.14)	Acc@5   0.00 (  0.00)
 * Acc@1 0.139 Acc@5 0.000
Test: [0/1]	Time  0.562 ( 0.562)	Loss 9.7433e-01 (9.7433e-01)	Acc@1   0.02 (  0.02)	Acc@5   0.00 (  0.00)
 * Acc@1 0.019 Acc@5 0.000
1
TRAINING
Epoch: [1][0/1]	Time  0.944 ( 0.944)	Data  0.213 ( 0.213)	Loss 1.6435e+00 (1.6435e+00)	Acc@1  -0.00 ( -0.00)	Acc@5   0.00 (  0.00)
[-0.001788141257419689]
Test: [0/1]	Time  0.216 ( 0.216)	Loss 2.4435e+00 (2.4435e+00)	Acc@1   0.14 (  0.14)	Acc@5   0.00 (  0.00)
 * Acc@1 0.143 Acc@5 0.000
Test: [0/1]	Time  0.278 ( 0.278)	Loss 9.0693e-01 (9.0693e-01)	Acc@1   0.02 (  0.02)	Acc@5   0.00 (  0.00)
 * Acc@1 0.023 Acc@5 0.000
2
TRAINING
Epoch: [2][0/1]	Time  0.942 ( 0.942)	Data  0.216 ( 0.216)	Loss 1.5343e+00 (1.5343e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
[0.004524878971756727]
Test: [0/1]	Time  0.215 ( 0.215)	Loss 2.4115e+00 (2.4115e+00)	Acc@1   0.15 (  0.15)	Acc@5   0.00 (  0.00)
 * Acc@1 0.151 Acc@5 0.000
Test: [0/1]	Time  0.274 ( 0.274)	Loss 8.4105e-01 (8.4105e-01)	Acc@1   0.03 (  0.03)	Acc@5   0.00 (  0.00)
 * Acc@1 0.027 Acc@5 0.000
3
TRAINING
Epoch: [3][0/1]	Time  0.959 ( 0.959)	Data  0.221 ( 0.221)	Loss 1.4228e+00 (1.4228e+00)	Acc@1   0.01 (  0.01)	Acc@5   0.00 (  0.00)
[0.013830316638463444]
Test: [0/1]	Time  0.218 ( 0.218)	Loss 2.3749e+00 (2.3749e+00)	Acc@1   0.16 (  0.16)	Acc@5   0.00 (  0.00)
 * Acc@1 0.161 Acc@5 0.000
Test: [0/1]	Time  0.272 ( 0.272)	Loss 7.9241e-01 (7.9241e-01)	Acc@1   0.03 (  0.03)	Acc@5   0.00 (  0.00)
 * Acc@1 0.033 Acc@5 0.000
4
TRAINING
Epoch: [4][0/1]	Time  0.960 ( 0.960)	Data  0.216 ( 0.216)	Loss 1.3309e+00 (1.3309e+00)	Acc@1   0.03 (  0.03)	Acc@5   0.00 (  0.00)
[0.03070470865003567]
Test: [0/1]	Time  0.217 ( 0.217)	Loss 2.3354e+00 (2.3354e+00)	Acc@1   0.17 (  0.17)	Acc@5   0.00 (  0.00)
 * Acc@1 0.169 Acc@5 0.000
Test: [0/1]	Time  0.273 ( 0.273)	Loss 7.6218e-01 (7.6218e-01)	Acc@1   0.04 (  0.04)	Acc@5   0.00 (  0.00)
 * Acc@1 0.039 Acc@5 0.000
5
TRAINING
Epoch: [5][0/1]	Time  0.946 ( 0.946)	Data  0.215 ( 0.215)	Loss 1.2588e+00 (1.2588e+00)	Acc@1   0.05 (  0.05)	Acc@5   0.00 (  0.00)
[0.0495047459145216]
Test: [0/1]	Time  0.235 ( 0.235)	Loss 2.2946e+00 (2.2946e+00)	Acc@1   0.20 (  0.20)	Acc@5   0.00 (  0.00)
 * Acc@1 0.201 Acc@5 0.000
Test: [0/1]	Time  0.275 ( 0.275)	Loss 7.4054e-01 (7.4054e-01)	Acc@1   0.05 (  0.05)	Acc@5   0.00 (  0.00)
 * Acc@1 0.046 Acc@5 0.000
6
TRAINING
Epoch: [6][0/1]	Time  0.977 ( 0.977)	Data  0.226 ( 0.226)	Loss 1.1915e+00 (1.1915e+00)	Acc@1   0.07 (  0.07)	Acc@5   0.00 (  0.00)
[0.0650290709684385]
Test: [0/1]	Time  0.219 ( 0.219)	Loss 2.2533e+00 (2.2533e+00)	Acc@1   0.25 (  0.25)	Acc@5   0.00 (  0.00)
 * Acc@1 0.250 Acc@5 0.000
Test: [0/1]	Time  0.273 ( 0.273)	Loss 7.1522e-01 (7.1522e-01)	Acc@1   0.06 (  0.06)	Acc@5   0.00 (  0.00)
 * Acc@1 0.058 Acc@5 0.000
7
TRAINING
Epoch: [7][0/1]	Time  0.949 ( 0.949)	Data  0.216 ( 0.216)	Loss 1.1116e+00 (1.1116e+00)	Acc@1   0.09 (  0.09)	Acc@5   0.00 (  0.00)
[0.08546480430389622]
Test: [0/1]	Time  0.224 ( 0.224)	Loss 2.2130e+00 (2.2130e+00)	Acc@1   0.30 (  0.30)	Acc@5   0.00 (  0.00)
 * Acc@1 0.298 Acc@5 0.000
Test: [0/1]	Time  0.277 ( 0.277)	Loss 6.7967e-01 (6.7967e-01)	Acc@1   0.07 (  0.07)	Acc@5   0.00 (  0.00)
 * Acc@1 0.068 Acc@5 0.000
8
TRAINING
Epoch: [8][0/1]	Time  0.948 ( 0.948)	Data  0.222 ( 0.222)	Loss 1.0114e+00 (1.0114e+00)	Acc@1   0.11 (  0.11)	Acc@5   0.00 (  0.00)
[0.10900642402812033]
Test: [0/1]	Time  0.222 ( 0.222)	Loss 2.1745e+00 (2.1745e+00)	Acc@1   0.35 (  0.35)	Acc@5   0.00 (  0.00)
 * Acc@1 0.353 Acc@5 0.000
Test: [0/1]	Time  0.277 ( 0.277)	Loss 6.3589e-01 (6.3589e-01)	Acc@1   0.08 (  0.08)	Acc@5   0.00 (  0.00)
 * Acc@1 0.077 Acc@5 0.000
9
TRAINING
Epoch: [9][0/1]	Time  0.958 ( 0.958)	Data  0.222 ( 0.222)	Loss 8.9658e-01 (8.9658e-01)	Acc@1   0.13 (  0.13)	Acc@5   0.00 (  0.00)
[0.13027589123723765]
Test: [0/1]	Time  0.218 ( 0.218)	Loss 2.1380e+00 (2.1380e+00)	Acc@1   0.40 (  0.40)	Acc@5   0.00 (  0.00)
 * Acc@1 0.397 Acc@5 0.000
Test: [0/1]	Time  0.274 ( 0.274)	Loss 5.9190e-01 (5.9190e-01)	Acc@1   0.09 (  0.09)	Acc@5   0.00 (  0.00)
 * Acc@1 0.086 Acc@5 0.000
10
TRAINING
Epoch: [10][0/1]	Time  0.958 ( 0.958)	Data  0.214 ( 0.214)	Loss 7.8164e-01 (7.8164e-01)	Acc@1   0.16 (  0.16)	Acc@5   0.00 (  0.00)
[0.15541456388136035]
Test: [0/1]	Time  0.215 ( 0.215)	Loss 2.1031e+00 (2.1031e+00)	Acc@1   0.43 (  0.43)	Acc@5   0.00 (  0.00)
 * Acc@1 0.429 Acc@5 0.000
Test: [0/1]	Time  0.267 ( 0.267)	Loss 5.5583e-01 (5.5583e-01)	Acc@1   0.10 (  0.10)	Acc@5   0.00 (  0.00)
 * Acc@1 0.096 Acc@5 0.000
11
TRAINING
Epoch: [11][0/1]	Time  0.965 ( 0.965)	Data  0.222 ( 0.222)	Loss 6.8099e-01 (6.8099e-01)	Acc@1   0.18 (  0.18)	Acc@5   0.00 (  0.00)
[0.17723411644424675]
Test: [0/1]	Time  0.219 ( 0.219)	Loss 2.0702e+00 (2.0702e+00)	Acc@1   0.47 (  0.47)	Acc@5   0.00 (  0.00)
 * Acc@1 0.468 Acc@5 0.000
Test: [0/1]	Time  0.277 ( 0.277)	Loss 5.3087e-01 (5.3087e-01)	Acc@1   0.11 (  0.11)	Acc@5   0.00 (  0.00)
 * Acc@1 0.107 Acc@5 0.000
12
TRAINING
Epoch: [12][0/1]	Time  1.004 ( 1.004)	Data  0.261 ( 0.261)	Loss 6.0153e-01 (6.0153e-01)	Acc@1   0.21 (  0.21)	Acc@5   0.00 (  0.00)
[0.20582070768426686]
Test: [0/1]	Time  0.218 ( 0.218)	Loss 2.0390e+00 (2.0390e+00)	Acc@1   0.53 (  0.53)	Acc@5   0.00 (  0.00)
 * Acc@1 0.528 Acc@5 0.000
Test: [0/1]	Time  0.269 ( 0.269)	Loss 5.1392e-01 (5.1392e-01)	Acc@1   0.12 (  0.12)	Acc@5   0.00 (  0.00)
 * Acc@1 0.118 Acc@5 0.000
13
TRAINING
Epoch: [13][0/1]	Time  1.022 ( 1.022)	Data  0.269 ( 0.269)	Loss 5.4056e-01 (5.4056e-01)	Acc@1   0.24 (  0.24)	Acc@5   0.00 (  0.00)
[0.2396809272833329]
Test: [0/1]	Time  0.216 ( 0.216)	Loss 2.0095e+00 (2.0095e+00)	Acc@1   0.56 (  0.56)	Acc@5   0.00 (  0.00)
 * Acc@1 0.563 Acc@5 0.000
Test: [0/1]	Time  0.272 ( 0.272)	Loss 4.9793e-01 (4.9793e-01)	Acc@1   0.13 (  0.13)	Acc@5   0.00 (  0.00)
 * Acc@1 0.133 Acc@5 0.000
14
TRAINING
Epoch: [14][0/1]	Time  0.998 ( 0.998)	Data  0.262 ( 0.262)	Loss 4.8921e-01 (4.8921e-01)	Acc@1   0.28 (  0.28)	Acc@5   0.00 (  0.00)
[0.27782778115172896]
Test: [0/1]	Time  0.218 ( 0.218)	Loss 1.9814e+00 (1.9814e+00)	Acc@1   0.59 (  0.59)	Acc@5   0.00 (  0.00)
 * Acc@1 0.593 Acc@5 0.000
Test: [0/1]	Time  0.274 ( 0.274)	Loss 4.7625e-01 (4.7625e-01)	Acc@1   0.15 (  0.15)	Acc@5   0.00 (  0.00)
 * Acc@1 0.151 Acc@5 0.000
15
TRAINING
slurmstepd: error: *** JOB 6279148 ON ng30103 CANCELLED AT 2022-05-27T02:41:20 ***
